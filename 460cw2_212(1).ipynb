{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "460cw2_212(1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FloraQin0325/deep_learning_CW2/blob/master/460cw2_212(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hhWTreQs-xXj"
      },
      "cell_type": "markdown",
      "source": [
        "# Coursework2: Convolutional Neural Networks "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L8jbPFKH-xXl"
      },
      "cell_type": "markdown",
      "source": [
        "## instructions"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qPZgBFOP-xXl"
      },
      "cell_type": "markdown",
      "source": [
        "Please submit a version of this notebook containing your answers **together with your trained model** on CATe as CW2.zip. Write your answers in the cells below each question.\n",
        "\n",
        "A PDF version of this notebook is also provided in case the figures do not render correctly.\n",
        "\n",
        "**The deadline for submission is 19:00, Thu 14th February, 2019**"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pMhnUXP6-xXm"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up working environment "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NKv8ORrK-xXn"
      },
      "cell_type": "markdown",
      "source": [
        "For this coursework you will need to train a large network, therefore we recommend you work with Google Colaboratory, which provides free GPU time. You will need a Google account to do so. \n",
        "\n",
        "Please log in to your account and go to the following page: https://colab.research.google.com. Then upload this notebook.\n",
        "\n",
        "For GPU support, go to \"Edit\" -> \"Notebook Settings\", and select \"Hardware accelerator\" as \"GPU\".\n",
        "\n",
        "You will need to install pytorch by running the following cell:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jzy7zWIMdbGM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "acjo8JP9-xXo",
        "outputId": "2e202cf9-0a88-4a0f-c1cb-e09bae30db39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 14.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "huJrqKTm-xXs"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YGcE1qLU-xXs"
      },
      "cell_type": "markdown",
      "source": [
        "For this coursework you will implement one of the most commonly used model for image recognition tasks, the Residual Network. The architecture is introduced in 2015 by Kaiming He, et al. in the paper [\"Deep residual learning for image recognition\"](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). \n",
        "<br>\n",
        "\n",
        "In a residual network, each block contains some convolutional layers, plus \"skip\" connections, which allow the activations to by pass a layer, and then be summed up with the activations of the skipped layer. The image below illustrates a building block in residual networks."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1MN79WiC-xXt"
      },
      "cell_type": "markdown",
      "source": [
        "![resnet-block](utils/resnet-block.png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d0LU5fjr-xXv"
      },
      "cell_type": "markdown",
      "source": [
        "Depending on the number of building blocks, resnets can have different architectures, for example ResNet-50, ResNet-101 and etc. Here you are required to build ResNet-18 to perform classification on the CIFAR-10 dataset, therefore your network will have the following architecture:"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1Njp58a6-xXw"
      },
      "cell_type": "markdown",
      "source": [
        "![resnet](utils/resnet.png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rg8EUXKS-xXx"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 (40 points)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MCi7SU5h-xXx"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, you will use basic pytorch operations to define the 2D convolution and max pooling operation. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3LV2pEzC-xXy"
      },
      "cell_type": "markdown",
      "source": [
        "### YOUR TASK"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aYBLt-av-xX0"
      },
      "cell_type": "markdown",
      "source": [
        "- implement the forward pass for Conv2D and MaxPool2D\n",
        "- You can only fill in the parts which are specified as \"YOUR CODE HERE\"\n",
        "- You are **NOT** allowed to use the torch.nn module and the conv2d/maxpooling functions in torch.nn.functional"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K0JLAagD6X4z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_5-rdG2d-xX3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Conv2D(nn.Module):\n",
        "    \n",
        "    def __init__(self, inchannel, outchannel, kernel_size, stride, padding, bias = True):\n",
        "        \n",
        "        super(Conv2D, self).__init__()\n",
        "        \n",
        "        self.inchannel = inchannel\n",
        "        self.outchannel = outchannel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(outchannel, inchannel, \n",
        "                                                 kernel_size, kernel_size))\n",
        "        self.weights.data.normal_(-0.1, 0.1)\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(outchannel, ))\n",
        "            self.bias.data.normal_(-0.1, 0.1)\n",
        "        else:\n",
        "            self.bias = None\n",
        "            \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        ##############################################################\n",
        "        #                       YOUR CODE HERE                       #       \n",
        "        ##############################################################\n",
        "        hout = ((x.shape[2] + 2 * self.padding - self.kernel_size)\n",
        "                //self.stride)+1\n",
        "        wout = ((x.shape[3] + 2 * self.padding - self.kernel_size)\n",
        "                //self.stride)+1\n",
        "\n",
        "        x_unf = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding\n",
        "                         , stride=self.stride)\n",
        "        if self.bias:\n",
        "            out_unf = (x_unf.transpose(1, 2).matmul(self.weights.view(\n",
        "                       self.weights.size(0), -1).t()).transpose(1, 2))+self.bias.view(-1, 1)\n",
        "        else:\n",
        "            out_unf = (x_unf.transpose(1, 2).matmul(self.weights.view(\n",
        "                       self.weights.size(0), -1).t()).transpose(1, 2))\n",
        "        output = out_unf.view(x.shape[0], self.outchannel, hout, wout)\n",
        "\n",
        "        ##############################################################\n",
        "        #                       END OF YOUR CODE                     #\n",
        "        ##############################################################\n",
        "      \n",
        "        \n",
        "\n",
        "        return output\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eI_dYNRC-xX5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MaxPool2D(nn.Module):\n",
        "    \n",
        "    def __init__(self, pooling_size):\n",
        "        # assume pooling_size = kernel_size = stride\n",
        "        \n",
        "        super(MaxPool2D, self).__init__()\n",
        "        \n",
        "        self.pooling_size = pooling_size\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "     \n",
        "        ##############################################################\n",
        "        #                       YOUR CODE HERE                       #       \n",
        "        ##############################################################\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        inchannel = x.shape[1]\n",
        "\n",
        "        x = F.unfold(x,kernel_size=self.pooling_size, stride=self.pooling_size)\n",
        "\n",
        "        x = x.reshape(batch_size,inchannel,self.pooling_size*self.pooling_size,-1)\n",
        "\n",
        "        x = x.max(dim=2)[0]\n",
        "\n",
        "        shape = torch.rand(1)\n",
        "\n",
        "        shape[0] = x.shape[0]*x.shape[1]*x.shape[2]\n",
        "        \n",
        "        shape =torch.sqrt(shape/(batch_size*inchannel))\n",
        "\n",
        "        output = x.reshape(batch_size,inchannel,int(shape),int(shape))\n",
        "      \n",
        "        \n",
        "        ##############################################################\n",
        "        #                       END OF YOUR CODE                     #\n",
        "        ##############################################################\n",
        "                \n",
        "        \n",
        "        return output\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R2bBhs3_6X5H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define resnet building blocks\n",
        "\n",
        "class ResidualBlock(nn.Module): \n",
        "    def __init__(self, inchannel, outchannel, stride=1): \n",
        "        \n",
        "        super(ResidualBlock, self).__init__() \n",
        "        \n",
        "        self.left = nn.Sequential(Conv2D(inchannel, outchannel, kernel_size=3, \n",
        "                                         stride=stride, padding=1, bias=False), \n",
        "                                  nn.BatchNorm2d(outchannel), \n",
        "                                  nn.ReLU(inplace=True), \n",
        "                                  Conv2D(outchannel, outchannel, kernel_size=3, \n",
        "                                         stride=1, padding=1, bias=False), \n",
        "                                  nn.BatchNorm2d(outchannel)) \n",
        "        \n",
        "        self.shortcut = nn.Sequential() \n",
        "        \n",
        "        if stride != 1 or inchannel != outchannel: \n",
        "            \n",
        "            self.shortcut = nn.Sequential(Conv2D(inchannel, outchannel, \n",
        "                                                 kernel_size=1, stride=stride, \n",
        "                                                 padding = 0, bias=False), \n",
        "                                          nn.BatchNorm2d(outchannel) ) \n",
        "            \n",
        "    def forward(self, x): \n",
        "        \n",
        "        out = self.left(x) \n",
        "        \n",
        "        out += self.shortcut(x) \n",
        "        \n",
        "        out = F.relu(out) \n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U-Sa0BAw6X5P",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define resnet\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, ResidualBlock, num_classes = 10):\n",
        "        \n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        self.inchannel = 64\n",
        "        self.conv1 = nn.Sequential(Conv2D(3, 64, kernel_size = 3, stride = 1,\n",
        "                                            padding = 1, bias = False), \n",
        "                                  nn.BatchNorm2d(64), \n",
        "                                  nn.ReLU())\n",
        "        \n",
        "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride = 1)\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride = 2)\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride = 2)\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride = 2)\n",
        "        self.maxpool = MaxPool2D(4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        \n",
        "    \n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "        \n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        for stride in strides:\n",
        "            \n",
        "            layers.append(block(self.inchannel, channels, stride))\n",
        "            \n",
        "            self.inchannel = channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "def ResNet18():\n",
        "    return ResNet(ResidualBlock)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yKcl5EfX-xYB"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 (40 points)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "y5X3jzo1-xYC"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, you will train the ResNet-18 defined in the previous part on the CIFAR-10 dataset. Code for loading the dataset, training and evaluation are provided. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SDsuvfh2-xYC"
      },
      "cell_type": "markdown",
      "source": [
        "### Your Task"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NyKiEvDG-xYE"
      },
      "cell_type": "markdown",
      "source": [
        "1. Train your network to achieve the best possible test set accuracy after a maximum of 10 epochs of training.\n",
        "\n",
        "2. You can use techniques such as optimal hyper-parameter searching, data pre-processing\n",
        "\n",
        "3. If necessary, you can also use another optimiser\n",
        "\n",
        "4. **Answer the following question:**\n",
        "Given such a network with a large number of trainable parameters, and a training set of a large number of data, what do you think is the best strategy for hyperparameter searching? "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1cJmwYJc-xYF"
      },
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER FOR 2.4 HERE**\n",
        "\n",
        "A:In my opnion, Bayesian Optimisation is the best strategy for hyperparameter searching. Since the strategy will create a proxy model of the true model and train the hyperparameters on a more cheaper proxy model and return a really good result. And Training may be very cheaper compared with other strategies. Besides that, the training cycles is less."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jx4EjUPa6X5d",
        "outputId": "8af0b926-4474-4004-ac1d-01b456d282dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "transform = T.ToTensor()\n",
        "##############################################################\n",
        "#                       YOUR CODE HERE                       #       \n",
        "##############################################################\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding = 4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "transform_val = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "transform_test = T.Compose([\n",
        "    \n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "# load data\n",
        "\n",
        "NUM_TRAIN = 49000\n",
        "print_every = 100\n",
        "\n",
        "\n",
        "data_dir = './data'\n",
        "cifar10_train = dset.CIFAR10(data_dir, train=True, download=True, transform=transform_train)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10(data_dir, train=True, download=True, transform=transform_val)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10(data_dir, train=False, download=True, transform=transform_test)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)\n",
        "##############################################################\n",
        "#                       END OF YOUR CODE HERE                       #       \n",
        "##############################################################\n",
        "\n",
        "USE_GPU = True\n",
        "dtype = torch.float32 \n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Lcscd7k66X5s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    # function for test accuracy on validation and test set\n",
        "    \n",
        "    if loader.dataset.train:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "##############################################################\n",
        "#                       YOUR CODE HERE                       #       \n",
        "##############################################################\n",
        "    return acc\n",
        "##############################################################\n",
        "#                       END OF YOUR CODE                     #\n",
        "##############################################################\n",
        "\n",
        "\n",
        "def train_part(model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        print(len(loader_train))\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters of the model using the gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print('Epoch: %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "                #check_accuracy(loader_val, model)\n",
        "                print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L4gYud8Np_MQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# code for optimising your network performance\n",
        "\n",
        "##############################################################\n",
        "#                       YOUR CODE HERE                       #       \n",
        "##############################################################\n",
        "\n",
        "#setting up the range of all the hyperparameters. using Bayesian Optimisation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rhsRFm2N0TTH",
        "outputId": "47983056-c09a-444e-933b-3fb19ea3263f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install GPy\n",
        "!pip install GPyOpt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/7d/e55ffc3b16b68e8b50ccecacec56715bcf49d5c2f204f5ba60374d419611/GPy-1.9.6.tar.gz (873kB)\n",
            "\u001b[K    100% |████████████████████████████████| 880kB 17.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/site-packages (from GPy) (1.15.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/site-packages (from GPy) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from GPy) (1.12.0)\n",
            "Collecting paramz>=0.9.0 (from GPy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/78/b0f0164a32518bfd3b98cb2e149b7a4d5504d13fb503b31a6c59b958ed18/paramz-0.9.4.tar.gz (70kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/site-packages (from paramz>=0.9.0->GPy) (4.3.0)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Running setup.py bdist_wheel for GPy ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/82/1d/32a361e1ff2b4d9129a60343831dd99cdc74440e2db1c55264\n",
            "  Running setup.py bdist_wheel for paramz ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a9/fc/74/3bbd263c43ed98d67343df24cebf0a0ee34afee40d769fda9c\n",
            "Successfully built GPy paramz\n",
            "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.9.6 paramz-0.9.4\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting GPyOpt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/40/ca8f080d74d9f4e29069faa944fcfb083e8693b6daaba0f1e4bc65c88650/GPyOpt-1.2.5.tar.gz (55kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/site-packages (from GPyOpt) (1.15.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/site-packages (from GPyOpt) (1.2.0)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.6/site-packages (from GPyOpt) (1.9.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from GPy>=1.8->GPyOpt) (1.12.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.6/site-packages (from GPy>=1.8->GPyOpt) (0.9.4)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/site-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.3.0)\n",
            "Building wheels for collected packages: GPyOpt\n",
            "  Running setup.py bdist_wheel for GPyOpt ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/33/1d/87/dc02440831ba986b1547dd11a7dcd44e893b0527083066d869\n",
            "Successfully built GPyOpt\n",
            "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: GPyOpt\n",
            "Successfully installed GPyOpt-1.2.5\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0ibudNFj0bwf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import GPy\n",
        "import GPyOpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "no8o-2VS6X5y",
        "outputId": "31821f04-67ed-4aee-ba1d-5f6f84d2c11e",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33848
        }
      },
      "cell_type": "code",
      "source": [
        "def objective(params):\n",
        "  params = params.squeeze()\n",
        "  model = ResNet18()\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                             lr = params[0], \n",
        "                             weight_decay = params[1]\n",
        "                           )\n",
        "  print(\"lr, weight_decay: \",params[0],params[1])\n",
        "  train_part(model, optimizer, epochs = 10)\n",
        "  acc = check_accuracy(loader_val, model)\n",
        "  return acc\n",
        "bounds = [{'name':'lr','type':'continuous','domain':(1e-5,4e-2)},\n",
        "          {'name':'weight_decay','type':'continuous','domain':(1e-5,4e-2)}     \n",
        "         ]\n",
        "max_iter = 3\n",
        "max_time = 100000000000\n",
        "X = np.zeros((1,2))\n",
        "X[0][0] = 0.001\n",
        "X[0][1] = 0.0002\n",
        "myProblem = GPyOpt.methods.BayesianOptimization(f = objective,X = X, domain = bounds, maximize = True)\n",
        "myProblem.run_optimization(max_iter=max_iter, max_time=max_time)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr, weight_decay:  0.001 0.0002\n",
            "766\n",
            "Epoch: 0, Iteration 0, loss = 2.3881\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 2.2943\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 2.3912\n",
            "\n",
            "Epoch: 0, Iteration 300, loss = 2.1228\n",
            "\n",
            "Epoch: 0, Iteration 400, loss = 2.0888\n",
            "\n",
            "Epoch: 0, Iteration 500, loss = 1.6861\n",
            "\n",
            "Epoch: 0, Iteration 600, loss = 2.0077\n",
            "\n",
            "Epoch: 0, Iteration 700, loss = 1.8688\n",
            "\n",
            "766\n",
            "Epoch: 1, Iteration 0, loss = 1.4942\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 1.7033\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 1.4706\n",
            "\n",
            "Epoch: 1, Iteration 300, loss = 1.3405\n",
            "\n",
            "Epoch: 1, Iteration 400, loss = 1.3695\n",
            "\n",
            "Epoch: 1, Iteration 500, loss = 1.4843\n",
            "\n",
            "Epoch: 1, Iteration 600, loss = 1.3124\n",
            "\n",
            "Epoch: 1, Iteration 700, loss = 0.9776\n",
            "\n",
            "766\n",
            "Epoch: 2, Iteration 0, loss = 1.0886\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 1.3372\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 0.9965\n",
            "\n",
            "Epoch: 2, Iteration 300, loss = 1.2475\n",
            "\n",
            "Epoch: 2, Iteration 400, loss = 0.8870\n",
            "\n",
            "Epoch: 2, Iteration 500, loss = 1.0062\n",
            "\n",
            "Epoch: 2, Iteration 600, loss = 0.8223\n",
            "\n",
            "Epoch: 2, Iteration 700, loss = 0.7185\n",
            "\n",
            "766\n",
            "Epoch: 3, Iteration 0, loss = 0.6992\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 1.0069\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 0.8881\n",
            "\n",
            "Epoch: 3, Iteration 300, loss = 0.6308\n",
            "\n",
            "Epoch: 3, Iteration 400, loss = 0.9445\n",
            "\n",
            "Epoch: 3, Iteration 500, loss = 0.7540\n",
            "\n",
            "Epoch: 3, Iteration 600, loss = 0.8057\n",
            "\n",
            "Epoch: 3, Iteration 700, loss = 1.0313\n",
            "\n",
            "766\n",
            "Epoch: 4, Iteration 0, loss = 0.7787\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 0.7333\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 0.7128\n",
            "\n",
            "Epoch: 4, Iteration 300, loss = 0.6606\n",
            "\n",
            "Epoch: 4, Iteration 400, loss = 0.7723\n",
            "\n",
            "Epoch: 4, Iteration 500, loss = 0.7408\n",
            "\n",
            "Epoch: 4, Iteration 600, loss = 0.5901\n",
            "\n",
            "Epoch: 4, Iteration 700, loss = 0.8007\n",
            "\n",
            "766\n",
            "Epoch: 5, Iteration 0, loss = 0.7952\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 0.7019\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 0.4165\n",
            "\n",
            "Epoch: 5, Iteration 300, loss = 0.6903\n",
            "\n",
            "Epoch: 5, Iteration 400, loss = 0.6339\n",
            "\n",
            "Epoch: 5, Iteration 500, loss = 0.4677\n",
            "\n",
            "Epoch: 5, Iteration 600, loss = 0.4711\n",
            "\n",
            "Epoch: 5, Iteration 700, loss = 0.5835\n",
            "\n",
            "766\n",
            "Epoch: 6, Iteration 0, loss = 0.4765\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 0.6818\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 0.5150\n",
            "\n",
            "Epoch: 6, Iteration 300, loss = 0.5365\n",
            "\n",
            "Epoch: 6, Iteration 400, loss = 0.8594\n",
            "\n",
            "Epoch: 6, Iteration 500, loss = 0.5770\n",
            "\n",
            "Epoch: 6, Iteration 600, loss = 0.4752\n",
            "\n",
            "Epoch: 6, Iteration 700, loss = 0.5795\n",
            "\n",
            "766\n",
            "Epoch: 7, Iteration 0, loss = 0.5759\n",
            "\n",
            "Epoch: 7, Iteration 100, loss = 0.3704\n",
            "\n",
            "Epoch: 7, Iteration 200, loss = 0.3392\n",
            "\n",
            "Epoch: 7, Iteration 300, loss = 0.5618\n",
            "\n",
            "Epoch: 7, Iteration 400, loss = 0.6053\n",
            "\n",
            "Epoch: 7, Iteration 500, loss = 0.6023\n",
            "\n",
            "Epoch: 7, Iteration 600, loss = 0.5952\n",
            "\n",
            "Epoch: 7, Iteration 700, loss = 0.4318\n",
            "\n",
            "766\n",
            "Epoch: 8, Iteration 0, loss = 0.5294\n",
            "\n",
            "Epoch: 8, Iteration 100, loss = 0.4897\n",
            "\n",
            "Epoch: 8, Iteration 200, loss = 0.3798\n",
            "\n",
            "Epoch: 8, Iteration 300, loss = 0.5580\n",
            "\n",
            "Epoch: 8, Iteration 400, loss = 0.6300\n",
            "\n",
            "Epoch: 8, Iteration 500, loss = 0.4359\n",
            "\n",
            "Epoch: 8, Iteration 600, loss = 0.4987\n",
            "\n",
            "Epoch: 8, Iteration 700, loss = 0.3898\n",
            "\n",
            "766\n",
            "Epoch: 9, Iteration 0, loss = 0.4385\n",
            "\n",
            "Epoch: 9, Iteration 100, loss = 0.2538\n",
            "\n",
            "Epoch: 9, Iteration 200, loss = 0.3927\n",
            "\n",
            "Epoch: 9, Iteration 300, loss = 0.4345\n",
            "\n",
            "Epoch: 9, Iteration 400, loss = 0.4950\n",
            "\n",
            "Epoch: 9, Iteration 500, loss = 0.5151\n",
            "\n",
            "Epoch: 9, Iteration 600, loss = 0.3612\n",
            "\n",
            "Epoch: 9, Iteration 700, loss = 0.4331\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 856 / 1000 correct (85.60)\n",
            "lr, weight_decay:  0.017834577999293603 0.007457413913763551\n",
            "766\n",
            "Epoch: 0, Iteration 0, loss = 2.4392\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 2.0446\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 1.8761\n",
            "\n",
            "Epoch: 0, Iteration 300, loss = 1.7988\n",
            "\n",
            "Epoch: 0, Iteration 400, loss = 1.9468\n",
            "\n",
            "Epoch: 0, Iteration 500, loss = 1.8385\n",
            "\n",
            "Epoch: 0, Iteration 600, loss = 1.7574\n",
            "\n",
            "Epoch: 0, Iteration 700, loss = 1.8708\n",
            "\n",
            "766\n",
            "Epoch: 1, Iteration 0, loss = 1.9474\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 1.8296\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 1.7774\n",
            "\n",
            "Epoch: 1, Iteration 300, loss = 1.6867\n",
            "\n",
            "Epoch: 1, Iteration 400, loss = 1.8905\n",
            "\n",
            "Epoch: 1, Iteration 500, loss = 1.7628\n",
            "\n",
            "Epoch: 1, Iteration 600, loss = 1.7996\n",
            "\n",
            "Epoch: 1, Iteration 700, loss = 1.9430\n",
            "\n",
            "766\n",
            "Epoch: 2, Iteration 0, loss = 1.8016\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 1.9070\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 2.0114\n",
            "\n",
            "Epoch: 2, Iteration 300, loss = 1.8763\n",
            "\n",
            "Epoch: 2, Iteration 400, loss = 1.7305\n",
            "\n",
            "Epoch: 2, Iteration 500, loss = 1.8701\n",
            "\n",
            "Epoch: 2, Iteration 600, loss = 1.6229\n",
            "\n",
            "Epoch: 2, Iteration 700, loss = 1.7842\n",
            "\n",
            "766\n",
            "Epoch: 3, Iteration 0, loss = 1.9137\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 1.9083\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 1.8817\n",
            "\n",
            "Epoch: 3, Iteration 300, loss = 1.8099\n",
            "\n",
            "Epoch: 3, Iteration 400, loss = 1.7005\n",
            "\n",
            "Epoch: 3, Iteration 500, loss = 1.8736\n",
            "\n",
            "Epoch: 3, Iteration 600, loss = 1.9258\n",
            "\n",
            "Epoch: 3, Iteration 700, loss = 1.7219\n",
            "\n",
            "766\n",
            "Epoch: 4, Iteration 0, loss = 1.6560\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 1.8191\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 1.7708\n",
            "\n",
            "Epoch: 4, Iteration 300, loss = 1.6862\n",
            "\n",
            "Epoch: 4, Iteration 400, loss = 1.8247\n",
            "\n",
            "Epoch: 4, Iteration 500, loss = 1.8352\n",
            "\n",
            "Epoch: 4, Iteration 600, loss = 1.7491\n",
            "\n",
            "Epoch: 4, Iteration 700, loss = 1.7931\n",
            "\n",
            "766\n",
            "Epoch: 5, Iteration 0, loss = 1.7749\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 1.9159\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 1.8238\n",
            "\n",
            "Epoch: 5, Iteration 300, loss = 1.5560\n",
            "\n",
            "Epoch: 5, Iteration 400, loss = 1.6737\n",
            "\n",
            "Epoch: 5, Iteration 500, loss = 1.8314\n",
            "\n",
            "Epoch: 5, Iteration 600, loss = 1.6986\n",
            "\n",
            "Epoch: 5, Iteration 700, loss = 1.7902\n",
            "\n",
            "766\n",
            "Epoch: 6, Iteration 0, loss = 1.7199\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 1.6432\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 1.8996\n",
            "\n",
            "Epoch: 6, Iteration 300, loss = 1.7594\n",
            "\n",
            "Epoch: 6, Iteration 400, loss = 1.7917\n",
            "\n",
            "Epoch: 6, Iteration 500, loss = 1.8570\n",
            "\n",
            "Epoch: 6, Iteration 600, loss = 1.7053\n",
            "\n",
            "Epoch: 6, Iteration 700, loss = 1.7111\n",
            "\n",
            "766\n",
            "Epoch: 7, Iteration 0, loss = 1.7725\n",
            "\n",
            "Epoch: 7, Iteration 100, loss = 1.8454\n",
            "\n",
            "Epoch: 7, Iteration 200, loss = 1.7216\n",
            "\n",
            "Epoch: 7, Iteration 300, loss = 1.7683\n",
            "\n",
            "Epoch: 7, Iteration 400, loss = 1.7379\n",
            "\n",
            "Epoch: 7, Iteration 500, loss = 1.7229\n",
            "\n",
            "Epoch: 7, Iteration 600, loss = 1.6953\n",
            "\n",
            "Epoch: 7, Iteration 700, loss = 1.7770\n",
            "\n",
            "766\n",
            "Epoch: 8, Iteration 0, loss = 1.8519\n",
            "\n",
            "Epoch: 8, Iteration 100, loss = 1.9043\n",
            "\n",
            "Epoch: 8, Iteration 200, loss = 1.7501\n",
            "\n",
            "Epoch: 8, Iteration 300, loss = 1.8312\n",
            "\n",
            "Epoch: 8, Iteration 400, loss = 1.7051\n",
            "\n",
            "Epoch: 8, Iteration 500, loss = 1.8243\n",
            "\n",
            "Epoch: 8, Iteration 600, loss = 1.8129\n",
            "\n",
            "Epoch: 8, Iteration 700, loss = 1.7334\n",
            "\n",
            "766\n",
            "Epoch: 9, Iteration 0, loss = 1.7949\n",
            "\n",
            "Epoch: 9, Iteration 100, loss = 1.6672\n",
            "\n",
            "Epoch: 9, Iteration 200, loss = 1.7302\n",
            "\n",
            "Epoch: 9, Iteration 300, loss = 1.8249\n",
            "\n",
            "Epoch: 9, Iteration 400, loss = 1.8436\n",
            "\n",
            "Epoch: 9, Iteration 500, loss = 1.7359\n",
            "\n",
            "Epoch: 9, Iteration 600, loss = 1.7087\n",
            "\n",
            "Epoch: 9, Iteration 700, loss = 1.8101\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 297 / 1000 correct (29.70)\n",
            "lr, weight_decay:  0.02128023555651738 0.03310391349836247\n",
            "766\n",
            "Epoch: 0, Iteration 0, loss = 2.5384\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 2.1381\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 2.1142\n",
            "\n",
            "Epoch: 0, Iteration 300, loss = 2.0413\n",
            "\n",
            "Epoch: 0, Iteration 400, loss = 2.1018\n",
            "\n",
            "Epoch: 0, Iteration 500, loss = 2.0743\n",
            "\n",
            "Epoch: 0, Iteration 600, loss = 2.2199\n",
            "\n",
            "Epoch: 0, Iteration 700, loss = 2.0292\n",
            "\n",
            "766\n",
            "Epoch: 1, Iteration 0, loss = 2.0157\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 2.0032\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 2.1634\n",
            "\n",
            "Epoch: 1, Iteration 300, loss = 1.9626\n",
            "\n",
            "Epoch: 1, Iteration 400, loss = 2.0697\n",
            "\n",
            "Epoch: 1, Iteration 500, loss = 2.1806\n",
            "\n",
            "Epoch: 1, Iteration 600, loss = 2.0892\n",
            "\n",
            "Epoch: 1, Iteration 700, loss = 2.0477\n",
            "\n",
            "766\n",
            "Epoch: 2, Iteration 0, loss = 2.1612\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 2.0031\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 2.0227\n",
            "\n",
            "Epoch: 2, Iteration 300, loss = 2.1827\n",
            "\n",
            "Epoch: 2, Iteration 400, loss = 2.1480\n",
            "\n",
            "Epoch: 2, Iteration 500, loss = 2.1399\n",
            "\n",
            "Epoch: 2, Iteration 600, loss = 2.0429\n",
            "\n",
            "Epoch: 2, Iteration 700, loss = 2.1571\n",
            "\n",
            "766\n",
            "Epoch: 3, Iteration 0, loss = 2.0057\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 1.9757\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 2.0267\n",
            "\n",
            "Epoch: 3, Iteration 300, loss = 2.1541\n",
            "\n",
            "Epoch: 3, Iteration 400, loss = 1.9918\n",
            "\n",
            "Epoch: 3, Iteration 500, loss = 2.1662\n",
            "\n",
            "Epoch: 3, Iteration 600, loss = 2.1523\n",
            "\n",
            "Epoch: 3, Iteration 700, loss = 2.1169\n",
            "\n",
            "766\n",
            "Epoch: 4, Iteration 0, loss = 2.1898\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 2.0425\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 2.1490\n",
            "\n",
            "Epoch: 4, Iteration 300, loss = 2.2228\n",
            "\n",
            "Epoch: 4, Iteration 400, loss = 2.1010\n",
            "\n",
            "Epoch: 4, Iteration 500, loss = 2.2432\n",
            "\n",
            "Epoch: 4, Iteration 600, loss = 2.2138\n",
            "\n",
            "Epoch: 4, Iteration 700, loss = 2.4889\n",
            "\n",
            "766\n",
            "Epoch: 5, Iteration 0, loss = 2.0269\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 2.0775\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 1.9776\n",
            "\n",
            "Epoch: 5, Iteration 300, loss = 2.0660\n",
            "\n",
            "Epoch: 5, Iteration 400, loss = 2.0357\n",
            "\n",
            "Epoch: 5, Iteration 500, loss = 2.1158\n",
            "\n",
            "Epoch: 5, Iteration 600, loss = 1.9955\n",
            "\n",
            "Epoch: 5, Iteration 700, loss = 2.1882\n",
            "\n",
            "766\n",
            "Epoch: 6, Iteration 0, loss = 2.1112\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 2.0269\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 2.0795\n",
            "\n",
            "Epoch: 6, Iteration 300, loss = 2.2339\n",
            "\n",
            "Epoch: 6, Iteration 400, loss = 2.0908\n",
            "\n",
            "Epoch: 6, Iteration 500, loss = 2.1337\n",
            "\n",
            "Epoch: 6, Iteration 600, loss = 2.2328\n",
            "\n",
            "Epoch: 6, Iteration 700, loss = 2.1409\n",
            "\n",
            "766\n",
            "Epoch: 7, Iteration 0, loss = 2.1239\n",
            "\n",
            "Epoch: 7, Iteration 100, loss = 2.1757\n",
            "\n",
            "Epoch: 7, Iteration 200, loss = 2.0848\n",
            "\n",
            "Epoch: 7, Iteration 300, loss = 2.1403\n",
            "\n",
            "Epoch: 7, Iteration 400, loss = 2.0619\n",
            "\n",
            "Epoch: 7, Iteration 500, loss = 2.1638\n",
            "\n",
            "Epoch: 7, Iteration 600, loss = 2.0667\n",
            "\n",
            "Epoch: 7, Iteration 700, loss = 2.2284\n",
            "\n",
            "766\n",
            "Epoch: 8, Iteration 0, loss = 2.2607\n",
            "\n",
            "Epoch: 8, Iteration 100, loss = 2.1481\n",
            "\n",
            "Epoch: 8, Iteration 200, loss = 2.1813\n",
            "\n",
            "Epoch: 8, Iteration 300, loss = 2.1537\n",
            "\n",
            "Epoch: 8, Iteration 400, loss = 2.1603\n",
            "\n",
            "Epoch: 8, Iteration 500, loss = 2.1250\n",
            "\n",
            "Epoch: 8, Iteration 600, loss = 2.2422\n",
            "\n",
            "Epoch: 8, Iteration 700, loss = 1.9941\n",
            "\n",
            "766\n",
            "Epoch: 9, Iteration 0, loss = 2.0961\n",
            "\n",
            "Epoch: 9, Iteration 100, loss = 2.0972\n",
            "\n",
            "Epoch: 9, Iteration 200, loss = 2.1126\n",
            "\n",
            "Epoch: 9, Iteration 300, loss = 2.1233\n",
            "\n",
            "Epoch: 9, Iteration 400, loss = 2.2548\n",
            "\n",
            "Epoch: 9, Iteration 500, loss = 2.1046\n",
            "\n",
            "Epoch: 9, Iteration 600, loss = 2.0936\n",
            "\n",
            "Epoch: 9, Iteration 700, loss = 2.1273\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 167 / 1000 correct (16.70)\n",
            "lr, weight_decay:  0.005536587641274029 0.008573682236848965\n",
            "766\n",
            "Epoch: 0, Iteration 0, loss = 2.7690\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 1.8514\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 1.8859\n",
            "\n",
            "Epoch: 0, Iteration 300, loss = 1.9022\n",
            "\n",
            "Epoch: 0, Iteration 400, loss = 2.0576\n",
            "\n",
            "Epoch: 0, Iteration 500, loss = 1.7686\n",
            "\n",
            "Epoch: 0, Iteration 600, loss = 1.8214\n",
            "\n",
            "Epoch: 0, Iteration 700, loss = 1.5686\n",
            "\n",
            "766\n",
            "Epoch: 1, Iteration 0, loss = 1.6034\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 1.7004\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 1.7954\n",
            "\n",
            "Epoch: 1, Iteration 300, loss = 1.7542\n",
            "\n",
            "Epoch: 1, Iteration 400, loss = 1.4746\n",
            "\n",
            "Epoch: 1, Iteration 500, loss = 1.4312\n",
            "\n",
            "Epoch: 1, Iteration 600, loss = 1.5744\n",
            "\n",
            "Epoch: 1, Iteration 700, loss = 1.7567\n",
            "\n",
            "766\n",
            "Epoch: 2, Iteration 0, loss = 1.4713\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 1.4428\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 1.3777\n",
            "\n",
            "Epoch: 2, Iteration 300, loss = 1.4059\n",
            "\n",
            "Epoch: 2, Iteration 400, loss = 1.4227\n",
            "\n",
            "Epoch: 2, Iteration 500, loss = 1.4099\n",
            "\n",
            "Epoch: 2, Iteration 600, loss = 1.4932\n",
            "\n",
            "Epoch: 2, Iteration 700, loss = 1.4789\n",
            "\n",
            "766\n",
            "Epoch: 3, Iteration 0, loss = 1.3623\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 1.6571\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 1.4958\n",
            "\n",
            "Epoch: 3, Iteration 300, loss = 1.3379\n",
            "\n",
            "Epoch: 3, Iteration 400, loss = 1.3158\n",
            "\n",
            "Epoch: 3, Iteration 500, loss = 1.5520\n",
            "\n",
            "Epoch: 3, Iteration 600, loss = 1.3223\n",
            "\n",
            "Epoch: 3, Iteration 700, loss = 1.4009\n",
            "\n",
            "766\n",
            "Epoch: 4, Iteration 0, loss = 1.3056\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 1.4363\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 1.3438\n",
            "\n",
            "Epoch: 4, Iteration 300, loss = 1.7402\n",
            "\n",
            "Epoch: 4, Iteration 400, loss = 1.3587\n",
            "\n",
            "Epoch: 4, Iteration 500, loss = 1.3709\n",
            "\n",
            "Epoch: 4, Iteration 600, loss = 1.4821\n",
            "\n",
            "Epoch: 4, Iteration 700, loss = 1.2025\n",
            "\n",
            "766\n",
            "Epoch: 5, Iteration 0, loss = 1.2099\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 1.6608\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 1.3638\n",
            "\n",
            "Epoch: 5, Iteration 300, loss = 1.2029\n",
            "\n",
            "Epoch: 5, Iteration 400, loss = 1.3706\n",
            "\n",
            "Epoch: 5, Iteration 500, loss = 1.5647\n",
            "\n",
            "Epoch: 5, Iteration 600, loss = 1.1215\n",
            "\n",
            "Epoch: 5, Iteration 700, loss = 1.3841\n",
            "\n",
            "766\n",
            "Epoch: 6, Iteration 0, loss = 1.2635\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 1.6351\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 1.3564\n",
            "\n",
            "Epoch: 6, Iteration 300, loss = 1.2573\n",
            "\n",
            "Epoch: 6, Iteration 400, loss = 1.2092\n",
            "\n",
            "Epoch: 6, Iteration 500, loss = 1.2614\n",
            "\n",
            "Epoch: 6, Iteration 600, loss = 1.2104\n",
            "\n",
            "Epoch: 6, Iteration 700, loss = 1.3539\n",
            "\n",
            "766\n",
            "Epoch: 7, Iteration 0, loss = 1.3994\n",
            "\n",
            "Epoch: 7, Iteration 100, loss = 1.2898\n",
            "\n",
            "Epoch: 7, Iteration 200, loss = 1.1996\n",
            "\n",
            "Epoch: 7, Iteration 300, loss = 1.3174\n",
            "\n",
            "Epoch: 7, Iteration 400, loss = 1.2342\n",
            "\n",
            "Epoch: 7, Iteration 500, loss = 1.4083\n",
            "\n",
            "Epoch: 7, Iteration 600, loss = 1.3606\n",
            "\n",
            "Epoch: 7, Iteration 700, loss = 1.2852\n",
            "\n",
            "766\n",
            "Epoch: 8, Iteration 0, loss = 1.3142\n",
            "\n",
            "Epoch: 8, Iteration 100, loss = 1.3008\n",
            "\n",
            "Epoch: 8, Iteration 200, loss = 1.2796\n",
            "\n",
            "Epoch: 8, Iteration 300, loss = 1.4519\n",
            "\n",
            "Epoch: 8, Iteration 400, loss = 1.2322\n",
            "\n",
            "Epoch: 8, Iteration 500, loss = 1.4468\n",
            "\n",
            "Epoch: 8, Iteration 600, loss = 1.1530\n",
            "\n",
            "Epoch: 8, Iteration 700, loss = 1.1635\n",
            "\n",
            "766\n",
            "Epoch: 9, Iteration 0, loss = 0.9494\n",
            "\n",
            "Epoch: 9, Iteration 100, loss = 1.2332\n",
            "\n",
            "Epoch: 9, Iteration 200, loss = 1.2744\n",
            "\n",
            "Epoch: 9, Iteration 300, loss = 1.3866\n",
            "\n",
            "Epoch: 9, Iteration 400, loss = 1.5888\n",
            "\n",
            "Epoch: 9, Iteration 500, loss = 1.1927\n",
            "\n",
            "Epoch: 9, Iteration 600, loss = 1.3250\n",
            "\n",
            "Epoch: 9, Iteration 700, loss = 1.1829\n",
            "\n",
            "Checking accuracy on validation set\n",
            "Got 369 / 1000 correct (36.90)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1lo9gEsSIaHa",
        "colab_type": "code",
        "colab": {},
        "outputId": "175ac2b0-95e6-4722-bb1f-5643daeb3d8f"
      },
      "cell_type": "code",
      "source": [
        "print(\"Best Hyperparameters: \", myProblem.x_opt)\n",
        "print(\"Best Accuracy: \",myProblem.fx_opt)\n",
        "myProblem.plot_convergence()\n",
        "##############################################################\n",
        "#                       END OF YOUR CODE HERE                       #       \n",
        "##############################################################\n",
        "\n",
        "\n",
        "# define and train the network\n",
        "\n",
        "model = ResNet18()\n",
        "bestParams = myProblem.x_opt\n",
        "optimizer = optim.Adam(model.parameters(), lr = bestParams[0], weight_decay = bestParams[1])\n",
        "\n",
        "train_part(model, optimizer, epochs = 10)\n",
        "\n",
        "\n",
        "# report test set accuracy\n",
        "\n",
        "check_accuracy(loader_test, model)\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Hyperparameters:  [0.001  0.0002]\n",
            "Best Accuracy:  -0.856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "766\n",
            "Epoch: 0, Iteration 0, loss = 2.7106\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 2.2778\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 2.2613\n",
            "\n",
            "Epoch: 0, Iteration 300, loss = 2.0905\n",
            "\n",
            "Epoch: 0, Iteration 400, loss = 2.0409\n",
            "\n",
            "Epoch: 0, Iteration 500, loss = 2.0135\n",
            "\n",
            "Epoch: 0, Iteration 600, loss = 1.8899\n",
            "\n",
            "Epoch: 0, Iteration 700, loss = 1.8358\n",
            "\n",
            "766\n",
            "Epoch: 1, Iteration 0, loss = 1.6023\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 1.6534\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 1.4879\n",
            "\n",
            "Epoch: 1, Iteration 300, loss = 1.5091\n",
            "\n",
            "Epoch: 1, Iteration 400, loss = 1.5850\n",
            "\n",
            "Epoch: 1, Iteration 500, loss = 1.1482\n",
            "\n",
            "Epoch: 1, Iteration 600, loss = 1.2440\n",
            "\n",
            "Epoch: 1, Iteration 700, loss = 1.1733\n",
            "\n",
            "766\n",
            "Epoch: 2, Iteration 0, loss = 1.2480\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 1.1660\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 1.3320\n",
            "\n",
            "Epoch: 2, Iteration 300, loss = 1.0468\n",
            "\n",
            "Epoch: 2, Iteration 400, loss = 0.9363\n",
            "\n",
            "Epoch: 2, Iteration 500, loss = 0.8560\n",
            "\n",
            "Epoch: 2, Iteration 600, loss = 0.8993\n",
            "\n",
            "Epoch: 2, Iteration 700, loss = 1.0031\n",
            "\n",
            "766\n",
            "Epoch: 3, Iteration 0, loss = 0.6398\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 1.0184\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 0.9299\n",
            "\n",
            "Epoch: 3, Iteration 300, loss = 0.9323\n",
            "\n",
            "Epoch: 3, Iteration 400, loss = 0.6531\n",
            "\n",
            "Epoch: 3, Iteration 500, loss = 0.8739\n",
            "\n",
            "Epoch: 3, Iteration 600, loss = 0.8476\n",
            "\n",
            "Epoch: 3, Iteration 700, loss = 0.6931\n",
            "\n",
            "766\n",
            "Epoch: 4, Iteration 0, loss = 0.7510\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 0.7403\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 0.6704\n",
            "\n",
            "Epoch: 4, Iteration 300, loss = 0.5437\n",
            "\n",
            "Epoch: 4, Iteration 400, loss = 0.6358\n",
            "\n",
            "Epoch: 4, Iteration 500, loss = 0.5224\n",
            "\n",
            "Epoch: 4, Iteration 600, loss = 0.7831\n",
            "\n",
            "Epoch: 4, Iteration 700, loss = 0.4541\n",
            "\n",
            "766\n",
            "Epoch: 5, Iteration 0, loss = 0.4747\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 0.6313\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 0.6025\n",
            "\n",
            "Epoch: 5, Iteration 300, loss = 0.3635\n",
            "\n",
            "Epoch: 5, Iteration 400, loss = 0.6116\n",
            "\n",
            "Epoch: 5, Iteration 500, loss = 0.6325\n",
            "\n",
            "Epoch: 5, Iteration 600, loss = 0.4841\n",
            "\n",
            "Epoch: 5, Iteration 700, loss = 0.4604\n",
            "\n",
            "766\n",
            "Epoch: 6, Iteration 0, loss = 0.5317\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 0.4976\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 0.5879\n",
            "\n",
            "Epoch: 6, Iteration 300, loss = 0.6389\n",
            "\n",
            "Epoch: 6, Iteration 400, loss = 0.7046\n",
            "\n",
            "Epoch: 6, Iteration 500, loss = 0.5182\n",
            "\n",
            "Epoch: 6, Iteration 600, loss = 0.6148\n",
            "\n",
            "Epoch: 6, Iteration 700, loss = 0.3974\n",
            "\n",
            "766\n",
            "Epoch: 7, Iteration 0, loss = 0.3819\n",
            "\n",
            "Epoch: 7, Iteration 100, loss = 0.3500\n",
            "\n",
            "Epoch: 7, Iteration 200, loss = 0.4960\n",
            "\n",
            "Epoch: 7, Iteration 300, loss = 0.5296\n",
            "\n",
            "Epoch: 7, Iteration 400, loss = 0.5977\n",
            "\n",
            "Epoch: 7, Iteration 500, loss = 0.4467\n",
            "\n",
            "Epoch: 7, Iteration 600, loss = 0.7439\n",
            "\n",
            "Epoch: 7, Iteration 700, loss = 0.4757\n",
            "\n",
            "766\n",
            "Epoch: 8, Iteration 0, loss = 0.3840\n",
            "\n",
            "Epoch: 8, Iteration 100, loss = 0.4964\n",
            "\n",
            "Epoch: 8, Iteration 200, loss = 0.4863\n",
            "\n",
            "Epoch: 8, Iteration 300, loss = 0.4777\n",
            "\n",
            "Epoch: 8, Iteration 400, loss = 0.3383\n",
            "\n",
            "Epoch: 8, Iteration 500, loss = 0.6338\n",
            "\n",
            "Epoch: 8, Iteration 600, loss = 0.4809\n",
            "\n",
            "Epoch: 8, Iteration 700, loss = 0.4399\n",
            "\n",
            "766\n",
            "Epoch: 9, Iteration 0, loss = 0.4134\n",
            "\n",
            "Epoch: 9, Iteration 100, loss = 0.4172\n",
            "\n",
            "Epoch: 9, Iteration 200, loss = 0.6042\n",
            "\n",
            "Epoch: 9, Iteration 300, loss = 0.4991\n",
            "\n",
            "Epoch: 9, Iteration 400, loss = 0.3542\n",
            "\n",
            "Epoch: 9, Iteration 500, loss = 0.2245\n",
            "\n",
            "Epoch: 9, Iteration 600, loss = 0.2673\n",
            "\n",
            "Epoch: 9, Iteration 700, loss = 0.3980\n",
            "\n",
            "Checking accuracy on test set\n",
            "Got 8327 / 10000 correct (83.27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A4tlP5xvDhM9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Part 3 (20 points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vmVxPYmk-xYW"
      },
      "cell_type": "markdown",
      "source": [
        "The code provided below will allow you to visualise the feature maps computed by different layers of your network. Run the code (install matplotlib if necessary) and **answer the following questions**: \n",
        "\n",
        "1. Compare the feature maps from low-level layers to high-level layers, what do you observe? \n",
        "\n",
        "2. Use the training log, reported test set accuracy and the feature maps, analyse the performance of your network. If you think the performance is sufficiently good, explain why; if not, what might be the problem and how can you improve the performance?\n",
        "\n",
        "3. What are the other possible ways to analyse the performance of your network?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ICbtFNag-xYX"
      },
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER FOR PART 3 HERE**\n",
        "\n",
        "1: the number of feature maps increases from low-level layers to high-level layers. And the low-level feature maps tends to be simpler like edge detection, etc, while the high level feature maps are more complex and begin to demonstrate the real features.\n",
        "\n",
        "2: I think the performance of my convolutional neural network is not sufficiently good. First of all, the accuracy is not high enough. I think adding number of epochs or number of bayesian optimisation max iterations may help to improve the performance. Besides that, maybe a different approach of dataset preprocessing e.g. kai_ming would improve the performance\n",
        "\n",
        "3: Confusion Matrix, Graphs of error rate"
      ]
    },
    {
      "metadata": {
        "id": "eKVt9OawIaHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "vis_labels = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
        "\n",
        "for l in vis_labels:\n",
        "\n",
        "    getattr(model, l).register_forward_hook(get_activation(l))\n",
        "    \n",
        "    \n",
        "data, _ = cifar10_test[0]\n",
        "data = data.unsqueeze_(0).to(device = device, dtype = dtype)\n",
        "\n",
        "output = model(data)\n",
        "\n",
        "\n",
        "\n",
        "for idx, l in enumerate(vis_labels):\n",
        "\n",
        "    act = activation[l].squeeze()\n",
        "\n",
        "    if idx < 2:\n",
        "        ncols = 8\n",
        "    else:\n",
        "        ncols = 32\n",
        "        \n",
        "    nrows = act.size(0) // ncols\n",
        "    \n",
        "    fig, axarr = plt.subplots(nrows, ncols)\n",
        "    fig.suptitle(l)\n",
        "\n",
        "\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            axarr[i, j].imshow(act[i * nrows + j].cpu())\n",
        "            axarr[i, j].axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "r9I7LtjB-xYe"
      },
      "cell_type": "markdown",
      "source": [
        "**=============== END OF CW2 ===============**"
      ]
    }
  ]
}